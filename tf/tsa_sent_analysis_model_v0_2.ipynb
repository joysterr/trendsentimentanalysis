{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "6_GZ0PxpZxss",
        "LQwRxWR1WbRc",
        "hq4nrSXiXboW",
        "--FIjK-lWmVQ",
        "V8NlYbzl2wCX",
        "_tNKlD73cjB7",
        "L0tbzzMbVzE5"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Sentiment Analysis model development v0.2\n",
        "###For TrendSentimentAnalysis (tsa.)\n",
        "Joyster Rodrigues | 2018487 "
      ],
      "metadata": {
        "id": "9Swzp7mWY9MS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Importing Dependencies & Datasets, etc."
      ],
      "metadata": {
        "id": "2iUlFvVvV_uP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ypl_nni7B4kG"
      },
      "outputs": [],
      "source": [
        "#imports\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import nltk\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "from gensim.models import Word2Vec, Phrases\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#import data\n",
        "dataset = pd.read_csv('/content/drive/MyDrive/AI/ML/datasets/imdb_dataset/IMDB Dataset.csv')\n",
        "print(dataset.head)"
      ],
      "metadata": {
        "id": "TPIKiLAIGXfk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(dataset['review']), len(dataset['sentiment']))"
      ],
      "metadata": {
        "id": "5E_X5ENRHYx5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Pre-processing dataset"
      ],
      "metadata": {
        "id": "6_GZ0PxpZxss"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sentiment_id(sentiment):\n",
        "  if sentiment == 'positive':\n",
        "    sent_id = 1\n",
        "  else:\n",
        "    sent_id = 0\n",
        "  return sent_id"
      ],
      "metadata": {
        "id": "fwHZLAcNUje1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_df = dataset.copy()\n",
        "data_df['sentiment_id'] = data_df['sentiment'].apply(lambda x: sentiment_id(x))"
      ],
      "metadata": {
        "id": "Xo3zz2rFT67t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#shuffle data\n",
        "data_df.sample"
      ],
      "metadata": {
        "id": "WfExvtQ-Vb3I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_df"
      ],
      "metadata": {
        "id": "C2sr59zXulo-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_df.describe()"
      ],
      "metadata": {
        "id": "zCl3ZwyAu5Zy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(data_df.review) "
      ],
      "metadata": {
        "id": "2R-sWfzfwI7K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#assign dataset & split\n",
        "split_data = round(len(data_df.review) * 0.80)\n",
        "x_train = data_df.review[0:split_data]\n",
        "y_train = data_df.sentiment_id[0:split_data]\n",
        "\n",
        "x_test = data_df.review[split_data:]\n",
        "y_test = data_df.sentiment_id[split_data:]"
      ],
      "metadata": {
        "id": "IuWuAOF-vxH0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(x_train),len(y_train), ' | ' ,len(x_test),len(y_test))"
      ],
      "metadata": {
        "id": "obOZD6i1xUlQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#preprocessing data\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "REPLACE_WITH_SPACE = re.compile(r'[^A-Za-z\\s]')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "GzufO6tUIp4m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "def clean_text(raw_input):\n",
        "  clean_text = BeautifulSoup(raw_input, 'lxml').get_text()\n",
        "\n",
        "  letters_only = REPLACE_WITH_SPACE.sub(' ', clean_text)\n",
        "\n",
        "  lowercase_only = letters_only.lower()\n",
        "\n",
        "  return lowercase_only\n",
        "\n",
        "\n",
        "def lemmatize(tokens):\n",
        "  tokens = list(map(lemmatizer.lemmatize, tokens))\n",
        "\n",
        "  lemmatized_tokens = list(map(lambda x: lemmatizer.lemmatize(x, 'v'), tokens))\n",
        "  \n",
        "  meaningful_words = list(filter(lambda x: not x in stop_words, lemmatized_tokens))\n",
        "  \n",
        "  return meaningful_words\n",
        "\n",
        "\n",
        "def pre_processing(review):\n",
        "  total = 0\n",
        "\n",
        "  review = clean_text(review)\n",
        "\n",
        "  tokens = word_tokenize(review)\n",
        "\n",
        "  lemmas = lemmatize(tokens)\n",
        "\n",
        "  return lemmas\n"
      ],
      "metadata": {
        "id": "26VO5gdo4vN5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train_clean = np.array(list(map(lambda x: pre_processing(x), x_train)))\n",
        "x_test_clean = np.array(list(map(lambda x: pre_processing(x), x_test)))"
      ],
      "metadata": {
        "id": "pLY0utY6Lm8Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ngrams transformation\n",
        "bigrams = Phrases(sentences= x_train_clean)\n",
        "trigrams = Phrases(sentences=bigrams[x_train_clean])"
      ],
      "metadata": {
        "id": "a0NpS2nsOcvN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bigram_model = Word2Vec(\n",
        "    sentences = bigrams[x_train_clean],\n",
        "    size = 256,\n",
        "    min_count=3, window=5, workers=4\n",
        ")"
      ],
      "metadata": {
        "id": "otEk_4HFUqs5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trigram_model = Word2Vec(\n",
        "    sentences = trigrams[bigrams[x_train_clean]],\n",
        "    size = 256,\n",
        "    min_count=3, window=5, workers=4\n",
        ")"
      ],
      "metadata": {
        "id": "JY_JeMn-RXr9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train_ngram = trigrams[bigrams[x_train_clean]]"
      ],
      "metadata": {
        "id": "Sw_IY8vJRXjd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#lemmatize & stemming\n",
        "#alternate method"
      ],
      "metadata": {
        "id": "UlWSmqgBIyGf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #text to vector / tokenization (Method1)\n",
        "# tokenizer = Tokenizer(num_words=10000, oov_token='<00V>')\n",
        "# tokenizer.fit_on_texts(x_train_clean)\n",
        "\n",
        "# word_index = tokenizer.word_index\n",
        "\n",
        "# x_train_seq = tokenizer.texts_to_sequences(x_train_clean)\n",
        "# x_train_pad = pad_sequences(x_train_seq, maxlen=150, padding='post', truncating='post')"
      ],
      "metadata": {
        "id": "Jq6W-r9oIt_A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#text to vector / tokenization (main)\n",
        "tokenizer1 = Tokenizer(num_words=10000, oov_token='<00V>')\n",
        "tokenizer1.fit_on_texts(x_train_ngram)\n",
        "\n",
        "word_index = tokenizer1.word_index\n",
        "\n",
        "x_train_seq = tokenizer1.texts_to_sequences(x_train_ngram)\n",
        "x_train_pad = pad_sequences(x_train_seq, maxlen=150, padding='post', truncating='post')"
      ],
      "metadata": {
        "id": "Sx1XVilXTm7N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_test_seq = tokenizer1.texts_to_sequences(x_test_clean)\n",
        "x_test_pad = pad_sequences(x_test_seq, maxlen=150, padding='post', truncating='post')"
      ],
      "metadata": {
        "id": "Vicq2z_21uJN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#np arrays for tf2.0+\n"
      ],
      "metadata": {
        "id": "U_0blR38XqVB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model Architecture v0.2\n",
        "Using LSTM model for sentiment analysis"
      ],
      "metadata": {
        "id": "LQwRxWR1WbRc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#model architecture (LSTM)\n",
        "from tensorflow.keras.layers import LSTM\n",
        "\n",
        "model1 = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(\n",
        "        input_dim = bigram_model.wv.vectors.shape[0],\n",
        "        output_dim = bigram_model.wv.vectors.shape[1],\n",
        "        input_length = 150,\n",
        "        weights = [bigram_model.wv.vectors],\n",
        "        trainable = False\n",
        "    ),\n",
        "    tf.keras.layers.Bidirectional(LSTM(128)),\n",
        "    tf.keras.layers.Dropout(rate = 0.25),\n",
        "    tf.keras.layers.Dense(64),\n",
        "    tf.keras.layers.Dropout(rate = 0.25),\n",
        "    tf.keras.layers.Dense(1, activation = 'sigmoid')\n",
        "])"
      ],
      "metadata": {
        "id": "G43dxpqrI2Iy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#compile model\n",
        "model1.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n"
      ],
      "metadata": {
        "id": "5bg0pkYj8wmZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model1.summary()"
      ],
      "metadata": {
        "id": "bEplI4cTXl5j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#fit on data\n",
        "ep0chs = 2\n",
        "hist = model1.fit(x_train_pad, y_train, epochs=ep0chs, validation_data=(x_test_pad, y_test))"
      ],
      "metadata": {
        "id": "c500gfJZYe8q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#model results"
      ],
      "metadata": {
        "id": "Q52YcB7Q8wdo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Evaluating model with quantitative + qualitative analysis"
      ],
      "metadata": {
        "id": "hq4nrSXiXboW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Model architecture diag."
      ],
      "metadata": {
        "id": "qrTw0w8DeRnL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.keras.utils.plot_model(\n",
        "    model1,\n",
        "    to_file='model1.png',\n",
        "    show_shapes=False,\n",
        "    show_dtype=False,\n",
        "    show_layer_names=True,\n",
        "    rankdir='TB',\n",
        "    expand_nested=False,\n",
        "    dpi=96,\n",
        "    layer_range=None,\n",
        "    show_layer_activations=False\n",
        ")"
      ],
      "metadata": {
        "id": "pMQa06Q5eQt8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##graphs of acc and val loss etc"
      ],
      "metadata": {
        "id": "nBbSEzawXo74"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##confusion matrix"
      ],
      "metadata": {
        "id": "OiAxKRcpXvej"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##sensitivity analysis"
      ],
      "metadata": {
        "id": "zgtTBE-iX0o-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Testing model"
      ],
      "metadata": {
        "id": "--FIjK-lWmVQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#model eval\n",
        "test_sub = []\n",
        "\n",
        "review1 = 'i am in love with this iPhone 12. It is such a beautiful piece of hardware and a sophisticated blend of software, haramonising together to give the best possible smartphone experience.'\n",
        "\n",
        "review2 = 'i cannot believe i spend so much money on this chair. the quality is poor, it could do much better. '\n",
        "\n",
        "test_sub.append(review1)\n",
        "test_sub.append(review2)"
      ],
      "metadata": {
        "id": "Cb3d8mdF82-Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_sub_clean = np.array(list(map(lambda x: pre_processing(x), test_sub)))\n",
        "test_sub_seq = tokenizer1.texts_to_sequences(test_sub_clean)\n",
        "test_sub_pad = pad_sequences(test_sub_seq, maxlen=150, padding='post', truncating='post')"
      ],
      "metadata": {
        "id": "aePXg75l_qP1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#def clean_tok_pad(test_sentences): \n",
        "#function like above"
      ],
      "metadata": {
        "id": "5dGLMuQNXGWl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model1.predict(test_sub_pad)"
      ],
      "metadata": {
        "id": "XuNxc6cx-6YS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def translate_emo(numeric_rep):\n",
        "  for item in numeric_rep:\n",
        "    if item > 0.5:\n",
        "      emo_out =[]\n",
        "      emo_out.append('positive')\n",
        "    if item < 0.5:\n",
        "      emo_out.append('negative')\n",
        "    return emo_out"
      ],
      "metadata": {
        "id": "u082kdlkBRUq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_output = model1.predict(test_sub_pad)\n",
        "print(model_output)"
      ],
      "metadata": {
        "id": "jDudU3c-CNu1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def emo(emo_in):\n",
        "  emo_out = []\n",
        "  for item in emo_in:\n",
        "    if item > 0.5:\n",
        "      emo_out.append('positive')\n",
        "    else:\n",
        "      emo_out.append('negative')\n",
        "  return emo_out"
      ],
      "metadata": {
        "id": "j8WHQX4wEqiA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(emo(model_output))"
      ],
      "metadata": {
        "id": "2oLjzGS5UVd2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#just more tests: quick"
      ],
      "metadata": {
        "id": "yM-LGD53Xb9q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_text = 'this is the worst day of my life. iOS 16 is killing my battery.'\n",
        "demo_only = []\n",
        "demo_only.append(example_text)\n",
        "out = tokenizer1.texts_to_sequences(example_text)\n",
        "out = pad_sequences(out)\n",
        "model_out2 = model1.predict(out)\n",
        "print(emo(model_out2))"
      ],
      "metadata": {
        "id": "SFWdrrFrYbuo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#(Testing) tsa performing sentiment analysis on recent 20 tweets"
      ],
      "metadata": {
        "id": "V8NlYbzl2wCX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_demo = pd.read_csv('/content/results.csv')\n",
        "print(df_demo)"
      ],
      "metadata": {
        "id": "oMYgGreZrLrE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "demo_test = []\n",
        "for item in df_demo['Tweet']:\n",
        "  item = re.sub(r'http\\S+', '', item)\n",
        "  demo_test.append(item)"
      ],
      "metadata": {
        "id": "1kUBs3FZrhtU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "demo_test"
      ],
      "metadata": {
        "id": "TyToOoscshm9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "demo_out_clean = np.array(list(map(lambda x: pre_processing(x), demo_test)))\n",
        "demo_out_seq = tokenizer1.texts_to_sequences(demo_out_clean)\n",
        "demo_out_pad = pad_sequences(demo_out_seq, maxlen=150, padding='post', truncating='post')"
      ],
      "metadata": {
        "id": "-eKyGkqJuYI_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "demo_results = model1.predict(demo_out_pad)\n",
        "print(emo(demo_results))"
      ],
      "metadata": {
        "id": "oNySUuWWu5BF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pos = 0\n",
        "neg = 0\n",
        "\n",
        "for item in demo_results:\n",
        "  if item >= 0.5:\n",
        "    pos += 1\n",
        "  else:\n",
        "    neg += 1\n"
      ],
      "metadata": {
        "id": "Wo25ZCtNzS5x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "zoJYlLklchlI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#tsa. development / integration"
      ],
      "metadata": {
        "id": "_tNKlD73cjB7"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "i_ofqHOHcpfp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Generating Graphs for tsa."
      ],
      "metadata": {
        "id": "TvPAQ0S5W3GO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##bar chart"
      ],
      "metadata": {
        "id": "e1Rj7F8jXCTL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sents = ['positive', 'negative']\n",
        "scores = [pos,neg]\n",
        "fig = plt.figure()\n",
        "bar = fig.add_axes([0,0,1,1])\n",
        "colors = ['green', 'red']\n",
        "bar.bar(sents, scores, color=colors)\n",
        "plt.title('iOS 16.3.1 tsa results')\n",
        "plt.xlabel('sentiments')\n",
        "plt.ylabel('score')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "HCzTbeJkvNiI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##pie chart"
      ],
      "metadata": {
        "id": "b8Ga_8FIXE-R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pie_labels = ['positive', 'negative']\n",
        "pie = plt.pie(scores, labels=pie_labels)\n",
        "plt.title('iOS 16.3.1 tsa results')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "UzAcLCxA19cS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##generate wordcloud"
      ],
      "metadata": {
        "id": "L0tbzzMbVzE5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import / \n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "df_demo = pd.read_csv('')\n",
        "df_demo.head"
      ],
      "metadata": {
        "id": "ve_mw2VQz1Ik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prep data:\n",
        "combi_string = ''\n",
        "\n",
        "for item in df_demo.CONTENT:\n",
        "  item = str(item)\n",
        "  tokens = item.split()\n",
        "\n",
        "  for i in range(len(tokens)):\n",
        "    tokens[i] = tokens[i].lower()\n",
        "\n",
        "  combi_string += \" \".join(tokens) + \" \""
      ],
      "metadata": {
        "id": "m1e87Dcr0RF7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from wordcloud import WordCloud, STOPWORDS\n",
        "\n",
        "stopwords = set(STOPWORDS)\n",
        "\n",
        "wordcloud = WordCloud(\n",
        "    width = 1000,\n",
        "    height = 1000,\n",
        "    background_color = 'white',\n",
        "    stopwords = stopwords,\n",
        "    min_font_size = 10\n",
        ").generate(combi_string)"
      ],
      "metadata": {
        "id": "iKWYOX_AwsIQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize = (8, 8), facecolor = None)\n",
        "plt.imshow(wordcloud)\n",
        "plt.axis('off')\n",
        "plt.tight_layout(pad = 0)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "qd8G2-9_yoFW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}